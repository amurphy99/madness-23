notes_23-01-06



forward propagation

use that formula TanH? i think



project structure:
-------------------

* folder > network classes code
* folder > data
* folder > data manipulation code
* folder > save states
* file   > run it

.git_ignore




team1 id
team2 id
team1 win (1 or 0)

team1 season averages for
team1 season averages against



  0: Season      1: DayNum      2: WTeamID     3: WScore      4: LTeamID     5: LScore      6: WLoc      
  7: NumOT       8: WFGM        9: WFGA       10: WFGM3      11: WFGA3      12: WFTM       13: WFTA      
 14: WOR        15: WDR        16: WAst       17: WTO        18: WStl       19: WBlk       20: WPF       
 21: LFGM       22: LFGA       23: LFGM3      24: LFGA3      25: LFTM       26: LFTA       27: LOR       
 28: LDR        29: LAst       30: LTO        31: LStl       32: LBlk       33: LPF 










Apply quantile transformation to transform the team_info to normal distribution.



























timer:  60.9480938911438
steps:  36
accuracy: 58.78%


timer:  181.18890476226807
steps:  109
accuracy: 60.11%







TODO
-----

* trim unused/old code

* turn output report into its own function

* more intelligent mutation overall
	- mutate the original copy of an agent as well as its clone

* try other math multiplecation thing

* create timers for everything and output a time report


* also score based on abs(of all weights)



Stat Changes
-------------

* change FGM, 3GFM, FTM to their percentages
	- leave attempted as is







what are all the things i can tweak
------------------------------------

* max weight
* max bias
* starting connections
* starting neurons
* starting layers

* weight mutation amount
* bias mutation amount
* chances for different things to mutate

* inverse tanh score values








matrix multiplecation:

each layer = list of connection weights



right now
----------
* size of numbers
	__setattr__(self, attr, value):
		if attr == 'bias':
			self.bias = int32(value)

* make sure if a neuron is deleted, 







use threading for progress bar
	after time limit reached say: 
	"time limit reached, finishing current step"
	"last step completed, running testing for report"




matrix multiplecation
----------------------

neurons gain:
	self.sending = [ list of neurons ]	
	self.weights = [list of connection weights] (numpy array)


def send_value(self):
	weighted_values = self.weights.matrix_multiply(self.value) 

	for i in range(len(self.sending)):
		self.sending[i].receiving += weighted_values[i]




functions
----------

* set_pointers

* erase_pointers
	- actually instead of erase pointers, just write a custom version of the copy() function
	- for neuron class copy function, just copy everything besides the sending list


def erase_pointers(self):
	# create a list of neuron (layers_index, index) addresses for each neuron in each layer
	for each neuron in each layer
		erase its sending list neurons and create a list of neuron addresses for each neuron in the main agent


def set_pointers(self):
	# go through the layer addresses list and re-assign the sending connections using the address list
	# when done, erase the list from both the original agent and the clone











for each column take the highest value, divide everything by that to get the "normalized" versions


change so game by game data was the teams average stats and average ranking AT THE TIME of that game
average ranking of opponents







layer 0 values = [ 0, 1, 2, 3, 4 ]
layer 0 weight = [ 0, 1, 2, 3, 4 ]
layer 1 values = [ 0, 1, 2, 3, 4 ] -> this line = previous two multipled by eachother

layer 1 weight = [ 0, 1, 2, 3, 4 ]
layer 2 values = [ 0, 1, 2, 3, 4 ]




eliminate outliers


inputs is each teams average stats at the time of the game
answers is the game stats













data creation is finished


* copy everything into a new folder




52 inputs

26 outputs











change to static # of connections
static # of layers
static # of neurons per layer


52 inputs
26 outputs

start with:
52
26
26
26
everything connected


layers
-------

layer[0]:
	52 long    - list of neuron values
	52*26 long - list of connection weights
	52*27 long - next layer values pre-op (including biases, each at the 0th index for their segment)

layer[1]:
	26 long    - list of neuron values
	26*26 long - list of connection weights
	26*27 long - next layer values pre-op (including biases, each at the 0th index for their segment)
...

layer[-1]:
	26 long - list of neuron values














64
16
16
16
10




















to do for website:
-------------------

* save network state
	- population object

* save tracking variables
	- timer
	- accuracy
	- cost
	- steps

* variable to end the training
	- and/or go by training for a set period of time instead of steps


* save data to a csv
	- inputs AND solutions files






try positive and negative
try with and without points











data needed


submission_inputs.csv


take team stats at the time of their final games
take every combo of team ids



ideas:
-------

winstreak
overtime games?


instead of their opponents average,
how did their opponent do compared to their average







new version with embeddings


4 embedding inputs
-------------------
coach, team
coach, team

data inputs
------------
location




right now i replace input neuron values at the start

pretty easily find the v-cost for the first input layer
update the embeddings there


2 new functions
batch size of 1
after every calculation, adjust the gradient descent, including the embeddings


inputs: 

steps/time, input_data, solutions, embeddings

input daya comes with inputs and tells what embeddings to slot in

function to create embeddings, function to update embeddings, 



team_name



stuff i could do:
------------------
* change embeddings from inputs to layer weights
* add coach embeddings in
* threading and ability to turn it off

* you can still do batches !
	- i think v_cost gets calculated every time, just ONLY adjust the embedding weights




stuff for website:
-------------------

encrypt
	* cookies
		- flask turorial covered this i think
	* database login/password
		- i had info on this from class, hopefully there is still a link in my class notes
	* escaping on sql inputs
		- there should be a tab saved with this stuff








Azure
DevOps
C#
Git
Bachelor's degree
JavaScript
SharePoint
TFS
Restaurant experience
TypeScript
Communication skills
Under 1 year















end